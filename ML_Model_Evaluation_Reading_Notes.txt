* After model deliver, it's better to build a evaluation metrics for online data to detect distribution drift

* Classification Metrics: accuracy, confusion matrix, pre-class accuracy, logloss, AUC/ROC
* Ranking Metrics: precision, recall, precision-recall curve, NDCG
* Regression Metrics: RMSE/RMSD, MAPE

— confusion matrix - when the consequence of each class can be different
— pre-class accuracy - when classes can be imbalanced
— logloss - output is numerical probability not discrete classes
— AUC summarizes ROC from a curve to a number 


— precision: predicted truly relevant/predicted to be relevant
— recall: predicted relevant/all real relevant 
— k: number of answers returned by the ranker
— precision-recall curve: plot precision versus recall over a range of k values
— F1 score = 2*precision*recall/(precision+recall)
* When there are multiple queries, average precision and recall, check average precision@k, average recall@k
* F1 score is the harmonic mean of precision and recall, it tends toward the smaller of the 2 numbers

— NDCG (normalized discounted cumulative gain)
* CG (cumulative gain): sums up the relevance of top k items
* DCG, NDCG are important in information retrieval where the positioning of the returned items is important

— RMSE/RMSD: The square root of the average squared distance between the actual score and predicted score. Since it is an average score, it is sensitive to large outliers
— MAPE: median absolute percentage, and use it to compute 90th percentile of the absolute percent error
